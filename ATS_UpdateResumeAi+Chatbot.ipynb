{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prtkmhn/ATS-Resume-Reviewer/blob/main/ATS_UpdateResumeAi%2BChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkBYh6KxGwnB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0ts4Fa2P7VZ",
        "outputId": "c6cba380-82ff-462b-ddc7-e5591fa3c16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 12s (405 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121885 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqJViidAGwnC"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio python-dotenv google-generativeai pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ycPt9PrvGwnD"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import base64\n",
        "import textwrap\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "luWUNluAGwnE"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "txt_model = genai.GenerativeModel('gemini-pro')\n",
        "vis_model = genai.GenerativeModel('gemini-pro-vision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "qDuEk0pXGwnF"
      },
      "outputs": [],
      "source": [
        "def extract_text(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    return pytesseract.image_to_string(image)\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return float(cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0])\n",
        "\n",
        "def update_resume(job_info, old_resume, similarity_score):\n",
        "    prompt = f\"\"\"Act Like a skilled or very experience ATS(Application Tracking System) with a deep understanding of tech field,software engineering,data science ,data analyst and big data engineer. Your task is to evaluate the resume based on the given job description. You must consider the job market is very competitive and you should provide\n",
        "best assistance for improving thw resumes. Based on the cosine similarity score of {similarity_score:.2f}, update the resume to better match the job description. Resume: {old_resume} Job Description: {job_info}\n",
        "I want the response as per below structure{{\"JD Match\": \"%\", \"MissingKeywords\": [], \"Profile Summary\": \"\"}}\"\"\"\n",
        "    response = txt_model.generate_content(prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "X0-yYzWdGwnH"
      },
      "outputs": [],
      "source": [
        "def process_application(image_path, resume_path):\n",
        "    job_text = extract_text(image_path)\n",
        "    with open(resume_path, 'r') as file:\n",
        "        resume_text = file.read()\n",
        "    similarity_score = calculate_cosine_similarity(job_text, resume_text)\n",
        "    updated_resume = update_resume(job_text, resume_text, similarity_score)\n",
        "    return job_text, str(similarity_score), updated_resume\n",
        "\n",
        "def image_to_base64(image_path):\n",
        "    with open(image_path, 'rb') as img:\n",
        "        encoded_string = base64.b64encode(img.read())\n",
        "    return encoded_string.decode('utf-8')\n",
        "\n",
        "def query_message(history, txt, img):\n",
        "    if not img:\n",
        "        history += [(txt, None)]\n",
        "        return history, \"\", history\n",
        "    base64 = image_to_base64(img)\n",
        "    data_url = f\"data:image/jpeg;base64,{base64}\"\n",
        "    history += [(f\"{txt} ![]({data_url})\", None)]\n",
        "    return history, \"\", history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_response(history, text, img):\n",
        "    if not img:\n",
        "        response = txt_model.generate_content(text)\n",
        "        history += [(None, response.text)]\n",
        "        return history, \"\", history\n",
        "    else:\n",
        "        img = Image.open(img)\n",
        "        response = vis_model.generate_content([text, img])\n",
        "        history += [(None, response.text)]\n",
        "        return history, \"\", history\n",
        "def initialize_chat_history(job_text, resume_text):\n",
        "    initial_prompt = f\"Here is the job description:\\n{job_text}\\n\\nAnd here is the resume:\\n{resume_text}\\n\\nPlease keep this information in mind as we proceed with our conversation.\"\n",
        "    response = txt_model.generate_content(initial_prompt)\n",
        "    return [(None, response.text)]\n"
      ],
      "metadata": {
        "id": "x7FbkTxtg-NH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Resume and Job Application Assistant\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"filepath\", label=\"Upload Job Description Image\")\n",
        "            resume_input = gr.File(type=\"filepath\", label=\"Upload Your Resume (txt format)\")\n",
        "            process_button = gr.Button(\"Process Application\")\n",
        "            outputs = [\n",
        "                gr.Textbox(label=\"Extracted Job Info\"),\n",
        "                gr.Number(label=\"Cosine Similarity Score\"),\n",
        "                gr.Textbox(label=\"Updated Resume\")\n",
        "            ]\n",
        "        with gr.Column():\n",
        "            image_box = gr.Image(type=\"filepath\", label=\"Upload Image for Chatbot\")\n",
        "            chatbot = gr.Chatbot(scale=2, height=750)\n",
        "            text_box = gr.Textbox(placeholder=\"Enter text and press enter, or upload an image\", container=False)\n",
        "            btn = gr.Button(\"Submit\")\n",
        "\n",
        "    job_text = gr.State()\n",
        "    resume_text = gr.State()\n",
        "    chat_history = gr.State([])\n",
        "\n",
        "    image_input.change(lambda x: extract_text(x), inputs=[image_input], outputs=[job_text])\n",
        "    resume_input.change(lambda x: open(x, 'r').read(), inputs=[resume_input], outputs=[resume_text])\n",
        "\n",
        "    process_button.click(process_application, inputs=[image_input, resume_input], outputs=outputs).then(\n",
        "        initialize_chat_history, inputs=[job_text, resume_text], outputs=[chat_history]\n",
        "    )\n",
        "\n",
        "    btn.click(query_message, [chat_history, text_box, image_box], [chat_history, text_box, chatbot]).then(\n",
        "        llm_response, [chat_history, text_box, image_box], [chat_history, text_box, chatbot]\n",
        "    )\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "GpPJahP5fY5O",
        "outputId": "02498399-ebaf-461b-9720-d5eff1f30508"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://d4b2bc19302d280b7c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d4b2bc19302d280b7c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d4b2bc19302d280b7c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}