{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv+TZNkykArxOnkKT3A+vf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prtkmhn/ATS-Resume-Reviewer/blob/prateek-development/ATS_TALK_SCREENSHOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5c65IgxGCOF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "import json\n",
        "from langchain.chains import LLMChain\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from pynput import keyboard\n",
        "import asyncio\n",
        "from asyncio import WindowsSelectorEventLoopPolicy\n",
        "import pyautogui\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from langchain.document_loaders import TextLoader\n",
        "import google.generativeai as genai\n",
        "\n",
        "asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())\n",
        "\n",
        "load_dotenv()\n",
        "os.environ['GOOGLE_API_KEY'] = \"YOUR API KEY\"\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "# Initialize Gemini Pro Vision model\n",
        "vision_model = genai.GenerativeModel('gemini-pro-vision')\n",
        "# Initialize Gemini Pro model\n",
        "gemini_model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "class LanguageModelProcessor:\n",
        "    def __init__(self, max_tokens=256, temperature=0.7):\n",
        "        self.llm = ChatGroq(temperature=temperature, model_name=\"llama3-70b-8192\", groq_api_key=os.getenv(\"GROQ_API_KEY\"), max_tokens=max_tokens)\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "        with open('system_prompt.txt', 'r') as file:\n",
        "            system_prompt = file.read().strip()\n",
        "\n",
        "        self.embed_model = \"text-embedding-3-small\"\n",
        "        self.embeddings = OpenAIEmbeddings(model=self.embed_model, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        self.documents = []\n",
        "\n",
        "        self.vector_store = None\n",
        "        self.retriever = None\n",
        "        self.retrieve_chain = None\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_template('''\n",
        "            Answer question based on provided context.\n",
        "            <context>\n",
        "            {context}\n",
        "            </context>\n",
        "\n",
        "            Question: {input}\n",
        "        ''')\n",
        "\n",
        "        self.document_chain = create_stuff_documents_chain(self.llm, self.prompt)\n",
        "\n",
        "        self.conversation_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "        ])\n",
        "\n",
        "        self.conversation = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=self.conversation_prompt,\n",
        "            memory=self.memory\n",
        "    )\n",
        "\n",
        "    def process(self, text):\n",
        "        self.memory.chat_memory.add_user_message(text)\n",
        "\n",
        "        start_time = time.time()\n",
        "        if self.retrieve_chain is not None:\n",
        "            response = self.retrieve_chain.invoke({\"input\": text})\n",
        "        else:\n",
        "            response = self.conversation.predict(text=text)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if isinstance(response, str):\n",
        "            response = {'answer': response}\n",
        "\n",
        "        self.memory.chat_memory.add_ai_message(response['answer'])\n",
        "\n",
        "        elapsed_time = int((end_time - start_time) * 1000)\n",
        "        print(f\"LLM ({elapsed_time}ms): {response['answer']}\")\n",
        "        return response['answer']\n",
        "\n",
        "\n",
        "class TextToSpeech:\n",
        "    def __init__(self):\n",
        "        self.engine = pyttsx3.init()\n",
        "\n",
        "    def speak(self, text):\n",
        "        self.engine.say(text)\n",
        "        self.engine.runAndWait()\n",
        "\n",
        "    def stop(self):\n",
        "        self.engine.stop()\n",
        "\n",
        "class SpeechToText:\n",
        "    def __init__(self):\n",
        "        self.recognizer = sr.Recognizer()\n",
        "\n",
        "    def listen(self):\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Listening...\")\n",
        "            audio = self.recognizer.listen(source)\n",
        "\n",
        "        try:\n",
        "            text = self.recognizer.recognize_google(audio)\n",
        "            print(f\"Human: {text}\")\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"Could not understand audio\")\n",
        "        except sr.RequestError as e:\n",
        "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, max_tokens=256, temperature=0.7):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.temperature = temperature\n",
        "        self.llm = LanguageModelProcessor(max_tokens=self.max_tokens, temperature=self.temperature)\n",
        "\n",
        "    def on_press(self, key):\n",
        "        if key == keyboard.KeyCode.from_char('x'):\n",
        "            self.stop_listening = True\n",
        "            self.tts.stop()\n",
        "            time.sleep(0.5)  # Add a small delay to ensure the speech is fully stopped\n",
        "            self.stt.listen()\n",
        "\n",
        "    async def take_screenshot(self):\n",
        "        screenshot = pyautogui.screenshot()\n",
        "        screenshot = pyautogui.screenshot(\"Tempimage.png\")\n",
        "        return screenshot\n",
        "\n",
        "    async def process_screenshot(self, screenshot):\n",
        "        response1 = vision_model.generate_content([\"Please extract the company name, job name, and job description from this image.\", screenshot], stream=True)\n",
        "        response1.resolve()\n",
        "        return response1.text\n",
        "\n",
        "    def add_text_file_to_embedding(self, file_path):\n",
        "        loader = TextLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        self.llm.documents.extend(text_splitter.split_documents(docs))\n",
        "        self.llm.vector_store = FAISS.from_documents(self.llm.documents, self.llm.embeddings)\n",
        "        self.llm.retriever = self.llm.vector_store.as_retriever()\n",
        "        self.llm.retrieve_chain = create_retrieval_chain(self.llm.retriever, self.llm.document_chain)\n",
        "\n",
        "    async def main(self):\n",
        "        self.tts = TextToSpeech()\n",
        "        self.stt = SpeechToText()\n",
        "\n",
        "        listener = keyboard.Listener(on_press=self.on_press)\n",
        "        listener.start()\n",
        "\n",
        "        while True:\n",
        "            input(\"Press Enter to start listening...\")\n",
        "            transcription_response = self.stt.listen()\n",
        "\n",
        "            if \"goodbye\" in transcription_response.lower():\n",
        "                break\n",
        "\n",
        "            if \"screenshot\" in transcription_response.lower():\n",
        "                screenshot = await self.take_screenshot()\n",
        "                processed_screenshot = await self.process_screenshot(screenshot)\n",
        "                print(\"Screenshot taken and processed.\")\n",
        "                print(f\"Extracted information: {processed_screenshot}\")\n",
        "                transcription_response = self.stt.listen()\n",
        "\n",
        "            if \"textfile\" in transcription_response.lower():\n",
        "                file_path = input(\"Enter the path to the text file: \")\n",
        "                self.add_text_file_to_embedding(file_path)\n",
        "                print(f\"Text file '{file_path}' added to the embedding.\")\n",
        "                continue\n",
        "\n",
        "            if transcription_response.strip():\n",
        "                llm_response = self.llm.process(transcription_response)\n",
        "                self.tts.speak(llm_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Talk to your LLM')\n",
        "    parser.add_argument('--max_tokens', type=int, default=70, help='Maximum number of tokens for LLM output')\n",
        "    parser.add_argument('--temperature', type=float, default=0.3, help='Temperature for LLM')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    manager = ConversationManager(max_tokens=args.max_tokens, temperature=args.temperature)\n",
        "    asyncio.run(manager.main())"
      ]
    }
  ]
}