{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqjjKEtaGgosRqyGsxxSwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prtkmhn/ATS-Resume-Reviewer/blob/prateek-development/ATS_TALK_SCREENSHOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5c65IgxGCOF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader, WebBaseLoader\n",
        "from langchain.chains import LLMChain\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from pynput import keyboard\n",
        "import asyncio\n",
        "from asyncio import WindowsSelectorEventLoopPolicy\n",
        "import pyautogui\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "\n",
        "asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())\n",
        "\n",
        "load_dotenv()\n",
        "os.environ['GOOGLE_API_KEY'] = \"YOUR API KEY\"\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "# Initialize Gemini Pro Vision model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize Gemini Pro Vision model\n",
        "vision_model = genai.GenerativeModel('gemini-pro-vision')\n",
        "# Initialize Gemini Pro model\n",
        "gemini_model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "class LanguageModelProcessor:\n",
        "    def __init__(self, max_tokens=256, temperature=0.7):\n",
        "        self.llm = ChatGroq(temperature=temperature, model_name=\"llama3-70b-8192\", groq_api_key=os.getenv(\"GROQ_API_KEY\"), max_tokens=max_tokens)\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "        with open('system_prompt.txt', 'r') as file:\n",
        "            system_prompt = file.read().strip()\n",
        "\n",
        "        self.embed_model = \"text-embedding-3-small\"\n",
        "        self.embeddings = OpenAIEmbeddings(model=self.embed_model, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        self.documents = []\n",
        "\n",
        "        self.vector_store = None\n",
        "        self.retriever = None\n",
        "        self.retrieve_chain = None\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_template('''\n",
        "            Here is some information extracted from a screenshot:\n",
        "            <context>\n",
        "            {context}\n",
        "            </context>\n",
        "            Based on this information and the user's resume, provide insights or suggestions that could be beneficial. Please ignore the context if no information is provided\n",
        "            If the conversation is not about the user's resume but in general about any other topic, please oblige and continue the conversation\n",
        "            Question: {input}\n",
        "        ''')\n",
        "\n",
        "        self.document_chain = create_stuff_documents_chain(self.llm, self.prompt)\n",
        "\n",
        "        self.conversation_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "        ])\n",
        "\n",
        "        self.conversation = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=self.conversation_prompt,\n",
        "            memory=self.memory\n",
        "        )\n",
        "\n",
        "    def process(self, text, screenshot_context=None):\n",
        "        self.memory.chat_memory.add_user_message(text)\n",
        "\n",
        "        start_time = time.time()\n",
        "        if screenshot_context:\n",
        "            # Use the screenshot context for \"on my screen\" requests\n",
        "            response = self.llm.predict(context=screenshot_context, input=text)\n",
        "        elif self.retrieve_chain is not None:\n",
        "            response = self.retrieve_chain.invoke({\"input\": text})\n",
        "        else:\n",
        "            response = self.conversation.predict(text=text)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if isinstance(response, str):\n",
        "            response = {'answer': response}\n",
        "\n",
        "        self.memory.chat_memory.add_ai_message(response['answer'])\n",
        "\n",
        "        elapsed_time = int((end_time - start_time) * 1000)\n",
        "        print(f\"LLM ({elapsed_time}ms): {response['answer']}\")\n",
        "        return response['answer']\n",
        "\n",
        "\n",
        "class TextToSpeech:\n",
        "    def __init__(self):\n",
        "        self.engine = pyttsx3.init()\n",
        "        self.stop_flag = False\n",
        "\n",
        "    def speak(self, text):\n",
        "        self.stop_flag = False\n",
        "        self.engine.say(text)\n",
        "        self.engine.runAndWait()\n",
        "        if self.stop_flag:\n",
        "            self.engine.stop()\n",
        "\n",
        "    def stop(self):\n",
        "        self.stop_flag = True\n",
        "\n",
        "class SpeechToText:\n",
        "    def __init__(self):\n",
        "        self.recognizer = sr.Recognizer()\n",
        "\n",
        "    def listen(self):\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Listening...\")\n",
        "            audio = self.recognizer.listen(source)\n",
        "\n",
        "        try:\n",
        "            text = self.recognizer.recognize_google(audio)\n",
        "            print(f\"Human: {text}\")\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"Could not understand audio\")\n",
        "        except sr.RequestError as e:\n",
        "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, max_tokens=256, temperature=0.3):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.temperature = temperature\n",
        "        self.llm = LanguageModelProcessor(max_tokens=self.max_tokens, temperature=self.temperature)\n",
        "\n",
        "    def on_press(self, key):\n",
        "        if key == keyboard.Key.enter:\n",
        "            self.tts.stop()\n",
        "        elif key.char == 'q':  # Change to the desired key for interruption\n",
        "            self.tts.stop()\n",
        "\n",
        "    async def take_screenshot(self):\n",
        "        screenshot = pyautogui.screenshot()\n",
        "        screenshot = pyautogui.screenshot(\"Tempimage.png\")\n",
        "        return screenshot\n",
        "\n",
        "    async def process_screenshot(self, screenshot):\n",
        "        response1 = vision_model.generate_content([\"Please extract the company name, job name, and job description from this image. Please be as detailed in extracting the information adding any supplementary information needed as well\", screenshot], stream=True)\n",
        "        response1.resolve()\n",
        "        extracted_info = response1.text\n",
        "\n",
        "        # Add the extracted information to Groq chat model's memory\n",
        "        self.llm.memory.chat_memory.add_user_message(extracted_info)\n",
        "\n",
        "        return extracted_info\n",
        "    async def on_my_screen(self):\n",
        "        screenshot = await self.take_screenshot()\n",
        "        processed_screenshot = await self.process_screenshot(screenshot)\n",
        "        print(\"Screenshot taken and processed.\")\n",
        "        print(f\"Extracted information: {processed_screenshot}\")\n",
        "\n",
        "        print(\"Please speak your prompt after the beep...\")\n",
        "        prompt = self.stt.listen()  # Use speech-to-text to capture the prompt\n",
        "\n",
        "        # Use the extracted information with the provided prompt\n",
        "        llm_response = self.llm.process(prompt, screenshot_context=processed_screenshot)\n",
        "        self.tts.speak(llm_response)\n",
        "        return llm_response\n",
        "\n",
        "    def add_text_file_to_embedding(self, file_path):\n",
        "        loader = TextLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        self.llm.documents.extend(text_splitter.split_documents(docs))\n",
        "        self.llm.vector_store = FAISS.from_documents(self.llm.documents, self.llm.embeddings)\n",
        "        self.llm.retriever = self.llm.vector_store.as_retriever()\n",
        "        self.llm.retrieve_chain = create_retrieval_chain(self.llm.retriever, self.llm.document_chain)\n",
        "\n",
        "    def add_url_to_embedding(self, url):\n",
        "        try:\n",
        "            loader = WebBaseLoader(url)\n",
        "            docs = loader.load()\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "            self.llm.documents.extend(text_splitter.split_documents(docs))\n",
        "            self.llm.vector_store = FAISS.from_documents(self.llm.documents, self.llm.embeddings)\n",
        "            self.llm.retriever = self.llm.vector_store.as_retriever()\n",
        "            self.llm.retrieve_chain = create_retrieval_chain(self.llm.retriever, self.llm.document_chain)\n",
        "            print(f\"Successfully added data from {url} to embeddings.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading URL: {url}\")\n",
        "            print(f\"Error message: {str(e)}\")\n",
        "\n",
        "    async def main(self):\n",
        "        self.tts = TextToSpeech()\n",
        "        self.stt = SpeechToText()\n",
        "\n",
        "        listener = keyboard.Listener(on_press=self.on_press)\n",
        "        listener.start()\n",
        "\n",
        "        # Add resume.txt to embeddings\n",
        "        self.add_text_file_to_embedding(\"resume.txt\")\n",
        "\n",
        "        # Load LinkedIn and GitHub profiles from file if available\n",
        "        if os.path.exists(\"profiles.txt\"):\n",
        "            with open(\"profiles.txt\", \"r\") as file:\n",
        "                linkedin_url = file.readline().strip()\n",
        "                github_url = file.readline().strip()\n",
        "        else:\n",
        "            linkedin_url = input(\"Enter your LinkedIn profile URL: \")\n",
        "            github_url = input(\"Enter your GitHub profile URL: \")\n",
        "            with open(\"profiles.txt\", \"w\") as file:\n",
        "                file.write(f\"{linkedin_url}\\n{github_url}\")\n",
        "\n",
        "        # Add LinkedIn profile to embeddings\n",
        "        self.add_url_to_embedding(linkedin_url)\n",
        "\n",
        "        # Add GitHub profile to embeddings\n",
        "        self.add_url_to_embedding(github_url)\n",
        "\n",
        "        while True:\n",
        "            input(\"Press Enter to start listening...\")\n",
        "            transcription_response = self.stt.listen()\n",
        "\n",
        "            if \"goodbye\" in transcription_response.lower():\n",
        "                break\n",
        "\n",
        "            if \"screenshot\" in transcription_response.lower():\n",
        "                screenshot = await self.take_screenshot()\n",
        "                processed_screenshot = await self.process_screenshot(screenshot)\n",
        "                print(\"Screenshot taken and processed.\")\n",
        "                print(f\"Extracted information: {processed_screenshot}\")\n",
        "\n",
        "                # Prompt user for confirmation to update resume\n",
        "                update_resume = input(\"Do you want to update your resume with the extracted job details? (y/n): \")\n",
        "                if update_resume.lower() == 'y':\n",
        "                    self.temp_resume_file.write(f\"\\n\\nNew Job Details:\\n{processed_screenshot}\")\n",
        "                    self.temp_resume_file.flush()  # Ensure data is written to the file\n",
        "                    print(\"Temporary resume updated with the new job details.\")\n",
        "                transcription_response = self.stt.listen()\n",
        "            if \"textfile\" in transcription_response.lower():\n",
        "                file_path = input(\"Enter the path to the text file: \")\n",
        "                self.add_text_file_to_embedding(file_path)\n",
        "                print(f\"Text file '{file_path}' added to the embedding.\")\n",
        "                continue\n",
        "            if \"on my screen\" in transcription_response.lower():\n",
        "                response = await self.on_my_screen()  # Now uses speech for the prompt\n",
        "                print(f\"Response based on screenshot: {response}\")\n",
        "\n",
        "            if transcription_response.strip():\n",
        "                llm_response = self.llm.process(transcription_response)\n",
        "                self.tts.speak(llm_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Talk to your LLM')\n",
        "    parser.add_argument('--max_tokens', type=int, default=70, help='Maximum number of tokens for LLM output')\n",
        "    parser.add_argument('--temperature', type=float, default=0.3, help='Temperature for LLM')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    manager = ConversationManager(max_tokens=args.max_tokens, temperature=args.temperature)\n",
        "    asyncio.run(manager.main())\n"
      ],
      "metadata": {
        "id": "QKdzB7zrm1im"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}